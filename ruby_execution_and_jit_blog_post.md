# How Ruby Executes Your Code: From Interpreter to JIT and Back

When you run Ruby code with JIT compilers (ZJIT or YJIT) enabled, it doesn't execute the same way throughout your application's lifetime. Ruby seamlessly switches between two execution modes: interpreting YARV bytecode and running native machine code generated by a JIT compiler.

This post focuses on how Ruby executes code, not how it compiles it. We'll explore three key execution flows: pure interpretation, transitioning from interpreter to JIT, and falling back from JIT to interpreter. While we use ZJIT (Ruby's experimental next-generation JIT) as our reference, these execution patterns apply equally to YJIT.

## Ruby's Building Blocks: ISEQs and Bytecode

### From Ruby Code to Bytecode

When Ruby loads your code, it doesn't execute the source directly. Instead, each method is compiled into an Instruction Sequence (ISEQ) - a data structure containing bytecode instructions that the Ruby VM can execute.

Let's look at a simple example:

```ruby
def foo
  bar()
end

def bar
  42
end
```

Running `ruby --dump=insn example.rb` shows us the bytecode:

```text
== disasm: #<ISeq:foo@example.rb:1 (1,0)-(3,3)>
0000 putself                                                          (   2)[LiCa]
0001 opt_send_without_block                 <calldata!mid:bar, argc:0, FCALL|VCALL|ARGS_SIMPLE>
0003 leave                                  [Re]

== disasm: #<ISeq:bar@example.rb:5 (5,0)-(7,3)>
0000 putobject                              42                        (   6)[LiCa]
0002 leave                                  [Re]
```

Each method becomes its own ISEQ with a sequence of bytecode instructions. The `foo` method has three instructions:
- `putself` - pushes `self` onto the stack
- `opt_send_without_block` - optimized method call to `bar`
- `leave` - returns from the method

### Where JIT Code Lives

The critical insight is that JIT-compiled code doesn't replace bytecode - it lives alongside it in the ISEQ structure. Here's what an ISEQ looks like internally:

```text
ISEQ (foo method)
├── body
│   ├── bytecode: [putself, opt_send_without_block, leave]
│   ├── jit_entry: NULL  // Initially no JIT code
│   └── jit_entry_calls: 0  // Call counter
```

After a method is called enough times and gets JIT-compiled:

```text
ISEQ (foo method)
├── body
│   ├── bytecode: [putself, opt_send_without_block, leave]  // Still here!
│   ├── jit_entry: 0x7f8b2c001000  // Pointer to native machine code
│   └── jit_entry_calls: 1000  // Reached compilation threshold
```

The `jit_entry` field is the gateway to native code. When it's NULL, Ruby interprets bytecode. When it points to compiled code, Ruby jumps directly to machine instructions.

### Key Bytecode Instructions

For understanding execution flow, these instructions are most important:

- `opt_send_without_block` - Optimized method call without a block argument
- `leave` - Returns from current method
- `opt_plus`, `opt_minus` - Optimized arithmetic operations

## Flow 1: Pure Interpreter Execution

When JIT is disabled or a method hasn't been compiled yet, Ruby executes bytecode through the interpreter. This is the baseline execution mode.

### The Interpreter Loop

The heart of Ruby's interpreter is `vm_exec_core()` (vm.c:2639), which implements a classic fetch-decode-execute loop:

1. **Fetch** - Read the next bytecode instruction at the current program counter
2. **Decode** - Determine what operation to perform
3. **Execute** - Perform the operation (might modify stack, call methods, etc.)
4. **Advance** - Move program counter to next instruction

This continues until the method returns or calls another method.

### Method Calls in Detail

When the interpreter encounters a method call like `opt_send_without_block`, here's what happens:

```text
┌─────────────────────────────────────────────────────┐
│              vm_exec_core()                          │
│                                                      │
│  Executing: [opt_send_without_block :bar]            │
│                    ↓                                 │
│         vm_sendish() (vm_insnhelper.c:6058)          │
│           1. Look up method 'bar'                    │
│           2. Set up bar() for execution              │
│           3. Return special value                    │
│                    ↓                                 │
│          JIT_EXEC check                              │
│           - Check if bar has JIT code                │
│           - In this case: no JIT code                │
│           - Continue interpreting                    │
│                    ↓                                 │
│        Continue with bar's bytecode                  │
└─────────────────────────────────────────────────────┘
```

The key insight: `vm_sendish()` doesn't execute the called method. It only sets up the method and returns a special value meaning "method ready to execute."

### The JIT_EXEC Check

Even in pure interpreter mode, Ruby always checks for JIT code after setting up a method call. The `JIT_EXEC` check appears after every `vm_sendish()` call:

```c
// After opt_send_without_block
val = vm_sendish(ec, cfp, cd, bh, mexp_search_method);
JIT_EXEC(ec, val);  // Always runs, even without JIT enabled
```

The `JIT_EXEC` logic (simplified):

```text
if (method_was_called && method_has_jit_code) {
    jump_to_jit_code();
} else {
    continue_interpreting();
}
```

When there's no JIT compilation:

- Checks if the method has a `jit_entry` (pointer to compiled code)
- Finds NULL (no compiled code)
- Continues with interpreter

### Method Returns

The `leave` instruction handles returns:

1. Clean up the current method's state
2. Pass the return value to the caller
3. Continue executing in the caller

The interpreter maintains full control throughout this process.

## Flow 2: Interpreter → JIT Transition

Now let's see what happens when a method has been JIT-compiled and the interpreter needs to transition to native code.

### The JIT_EXEC Gateway

The same `JIT_EXEC` check we saw earlier becomes the gateway to JIT code. After `vm_sendish()` sets up a method, `JIT_EXEC` checks if that method has been compiled:

```text
JIT_EXEC logic:
if (method_was_set_up &&
    new_method_called &&
    method_has_jit_code) {
    jump_to_native_code();
}
```

All three conditions must be true:

1. **Method was set up** - vm_sendish returned a special value
2. **New method called** - Not a tail call optimization
3. **Method has JIT code** - `jit_compile()` finds compiled code

### How Methods Get JIT Compiled

Methods don't start with JIT code. They earn it through usage. ZJIT uses a two-phase approach:

```c
if (body->jit_entry == NULL && rb_zjit_enabled_p) {
    body->jit_entry_calls++;

    // First phase: Profile at threshold
    if (body->jit_entry_calls == rb_zjit_profile_threshold) {
        rb_zjit_profile_iseq_entry(iseq, ec);  // Gather runtime info
    }
    // Second phase: Compile at higher threshold
    else if (body->jit_entry_calls == rb_zjit_compile_threshold) {
        rb_zjit_compile_iseq(iseq, ec, false);  // Generate native code
        // After this, iseq->body->jit_entry points to compiled code
    }
}
```

This two-phase approach allows the JIT to:

1. Profile execution patterns before compilation
2. Generate optimized code based on actual runtime behavior

### The Transition Sequence

Let's trace through an actual interpreter-to-JIT transition:

```text
Interpreter executing foo() [not JITted]:

1. Executes: opt_send_without_block :bar

2. vm_sendish() runs:
   - Looks up bar method
   - Sets up bar for execution
   - Returns special value

3. JIT_EXEC check runs

4. jit_compile() runs:
   - Gets bar's ISEQ
   - Checks iseq->body->jit_entry
   - Finds 0x7f8b2c001000 (pointer to compiled code)
   - Returns this function pointer

5. Direct jump to native code:
   - Call function at 0x7f8b2c001000
   - CPU executes compiled instructions
   - No more bytecode interpretation
```

### Visual: ISEQ State During Transition

Before the call to bar:

```text
ISEQ (bar)
├── body
│   ├── jit_entry: 0x7f8b2c001000  ← JIT code exists!
│   ├── jit_entry_calls: 1000       ← Reached threshold
│   └── bytecode: [putobject 42, leave]  ← Still available
```

During the transition:

```text
Method:              Execution Mode:
foo()          →     Interpreter (executing opt_send_without_block)
     ↓
bar()          →     JIT (about to execute native code at 0x7f8b2c001000)
```

The transition is remarkably simple - it's just a function pointer call. The interpreter jumps directly to the machine code address stored in `jit_entry`.

### What Happens in JIT Code

Once execution transfers to JIT code, native CPU instructions execute directly. Here's a glimpse of what the `bar` method looks like after JIT compilation (ARM64 assembly):

```asm
# bb0(v0): block in <main>@/tmp/test_jit.rb:6
0x1200c0084: stp x29, x30, [sp, #-0x10]!
0x1200c0088: mov x29, sp

# Guard: check object class matches expected type
0x1200c00a0: tst x0, #7
0x1200c00a4: b.ne #0x1200c0188     # side exit if guard fails

# Direct method call to 'bar' (no method lookup!)
0x1200c00e4: stur x0, [x21]
0x1200c00e8: ldr x1, #0x1200c00f0
0x1200c00f8: stur x1, [x21, #8]

# Push new control frame
0x1200c011c: stur x1, [x19, #-0x28]
0x1200c0120: stur x0, [x19, #-0x20]
```

Key differences from interpreter execution:

- Native CPU instructions execute directly
- No instruction fetch/decode overhead
- Guards ensure type safety with conditional branches
- Direct jumps to other JIT code (no method lookup)
- Register allocation instead of stack manipulation

The JIT code has full access to the Ruby VM's data structures and can:

- Call other JITted methods directly
- Fall back to the interpreter when needed (via side exits)
- Manipulate VM state just like the interpreter would

## Flow 3: JIT → Interpreter (Deoptimization)

JIT code can't always continue executing natively. When it encounters situations it can't handle, it gracefully returns control to the interpreter. The most common reason is guard failures.

### Type Guards and Optimized Code

JIT compilers optimize code by making assumptions. Consider this method:

```ruby
def add(a, b)
  a + b
end
```

The bytecode uses `opt_plus`:

```text
== disasm: #<ISeq:add@-e:1 (1,0)-(1,25)>
0000 getlocal_WC_0                          a@0
0002 getlocal_WC_0                          b@1
0004 opt_plus                               <calldata!mid:+, argc:1>
0006 leave
```

When ZJIT compiles this method after profiling shows it's always called with Fixnums (small integers), it generates optimized code:

```c
// Simplified representation of what ZJIT generates:
// 1. Type guard for 'a'
if (!FIXNUM_P(a)) goto side_exit;

// 2. Type guard for 'b'
if (!FIXNUM_P(b)) goto side_exit;

// 3. Fast path: direct integer addition
result = INT2FIX(FIX2INT(a) + FIX2INT(b));
return result;
```

The JIT inserts type guards using instructions like:

```rust
Insn::GuardType { val, guard_type: types::Fixnum, state }
```

### Guard Failure Flow

What happens when assumptions break:

```ruby
# First 30 calls - guards pass
add(1, 2)      # Both Fixnums ✓
add(5, 10)     # Both Fixnums ✓

# Call 31 - guard fails!
add(1.5, 2)    # 1.5 is Float, not Fixnum ✗
```

When the type guard fails:

```text
JIT Code (add method):
├─ Load 'a' (value: 1.5)
├─ Check: is 'a' Fixnum?
│  └─ NO! It's a Float
├─ Jump to side exit ←── Deoptimization starts here
│
Side Exit:
├─ Restore interpreter state:
│  ├─ Update stack pointer
│  ├─ Update instruction pointer
│  └─ Signal "need interpreter"
└─ Return to interpreter
```

### The VM_EXEC Return Path

When JIT code returns control to the interpreter, it uses the `VM_EXEC` logic:

```text
VM_EXEC logic:
if (jit_returned_special_value) {
    mark_for_interpreter_execution();
    return_to_interpreter();
}
```

The process:

1. JIT returns a special value signaling "need interpreter"
2. Set flags for interpreter execution
3. Call `vm_exec()` to continue in interpreter mode

### Visual: Complete Deoptimization Flow

```text
Initial State:
  JIT executing add(1.5, 2)
         ↓
Type Guard Check:
  FIXNUM_P(1.5) → false
         ↓
Side Exit:
  ├─ Save registers
  ├─ Restore instruction pointer to opt_plus
  ├─ Restore stack state
  └─ Signal "need interpreter"
         ↓
VM_EXEC logic:
  Check for special return value
  Call vm_exec() to re-enter interpreter
         ↓
Interpreter:
  Executes opt_plus with Float + Fixnum
  (slower but correct path)
```

### Other Deoptimization Triggers

Besides type guards, JIT returns to interpreter for:

- **Calling non-JITted methods** - If the target method hasn't been compiled
- **Unsupported bytecode**
- **Redefined core methods (BOP)** - If someone redefines `+` on Integer
- **TracePoint activation** - When TracePoint is enabled, Ruby falls back to executing the bytecode only to make sure TracePoint events are properly triggered
- **Ractor activation** - Some operations, like reading class/module's instance variables, have different behaviour under multi-ractor environments. So when a non-main ractor is spawned, JIT compilers would deoptimize related code as well.

The key insight: deoptimization isn't failure - it's a normal part of Ruby's execution strategy. The VM seamlessly handles the transition, ensuring your code always runs correctly even when optimistic assumptions prove wrong.

## ZJIT Compilation Timeline

While this post focuses on execution instead of compilation, I think understanding the general idea of compilation timeline and thresholds will help you understand the execution too.

Here's how a typical method evolves from interpreted to JIT-compiled with ZJIT enabled:

```text
Method 'calculate' lifecycle:

Calls:     0 ──────── 25 ──────── 30 ─────────────────►
           │          │           │
Mode:      └─Interpret┴─Profile──┴─Native Code (JIT compiled)

Timeline:  Cold start  Gathering  Optimization complete,
                      type info   executing native code
```

Key thresholds (ZJIT defaults, may change in the future):

- **Profile threshold**: 25 calls - Start gathering type information (5 calls of profiling)
- **Compile threshold**: 30 calls - Generate optimized native code
- **Steady state**: 30+ calls - Execute compiled code

The specific numbers here is not important now and will likely change in the future.

What's important is that this is why JIT compilers usually need time to "warm up" to get to
the optimal speed.

## Takeaways

Understanding Ruby's execution flow reveals three key insights:

1. **JIT code lives alongside bytecode, not replacing it** - Every ISEQ maintains both its original bytecode and a `jit_entry` pointer to native code. The `JIT_EXEC` check after every method call decides which to execute.

2. **Transitions are seamless and bidirectional** - Ruby can jump from interpreter to JIT (via function pointer), and JIT can return to interpreter (via side exits). Both happen automatically without your code knowing.

3. **Guards enable optimization without sacrificing correctness** - JIT code makes assumptions (like "this will be a Fixnum") protected by guards. When assumptions break, deoptimization ensures your code still runs correctly, just interpreted.

The next time your Ruby application runs in production with JIT enabled, you'll know exactly how it's transitioning between interpreted bytecode and native machine code - potentially thousands of times per second, all transparently managed by the Ruby VM.
