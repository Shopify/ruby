# How Ruby Executes Your Code: From Interpreter to JIT and Back

When you run Ruby code in production, it doesn't execute the same way throughout your application's lifetime. Ruby seamlessly switches between two execution modes: interpreting bytecode and running native machine code generated by a JIT compiler. Understanding this execution flow helps explain Ruby's performance characteristics and why certain optimizations matter.

This post focuses on how Ruby executes code, not how it compiles it. We'll explore three key execution flows: pure interpretation, transitioning from interpreter to JIT, and falling back from JIT to interpreter. While we use ZJIT (Ruby's experimental next-generation JIT) as our reference, these execution patterns apply equally to YJIT.

Note that JIT compilation is typically disabled in development environments and enabled in production, so your local development experience might differ from production behavior.

## Ruby's Building Blocks: ISEQs and Bytecode

### From Ruby Code to Bytecode

When Ruby loads your code, it doesn't execute the source directly. Instead, each method is compiled into an Instruction Sequence (ISEQ) - a data structure containing bytecode instructions that the Ruby VM can execute.

Let's look at a simple example:

```ruby
def foo
  bar()
end

def bar
  42
end
```

Running `ruby --dump=insn example.rb` shows us the bytecode:

```
== disasm: #<ISeq:foo@example.rb:1 (1,0)-(3,3)>
0000 putself                                                          (   2)[LiCa]
0001 opt_send_without_block                 <calldata!mid:bar, argc:0, FCALL|VCALL|ARGS_SIMPLE>
0003 leave                                  [Re]

== disasm: #<ISeq:bar@example.rb:5 (5,0)-(7,3)>
0000 putobject                              42                        (   6)[LiCa]
0002 leave                                  [Re]
```

Each method becomes its own ISEQ with a sequence of bytecode instructions. The `foo` method has three instructions:
- `putself` - pushes `self` onto the stack
- `opt_send_without_block` - optimized method call to `bar`
- `leave` - returns from the method

### Where JIT Code Lives

The critical insight is that JIT-compiled code doesn't replace bytecode - it lives alongside it in the ISEQ structure. Here's what an ISEQ looks like internally:

```
ISEQ (foo method)
├── body
│   ├── bytecode: [putself, opt_send_without_block, leave]
│   ├── jit_entry: NULL  // Initially no JIT code
│   └── jit_entry_calls: 0  // Call counter
```

After a method is called enough times and gets JIT-compiled:

```
ISEQ (foo method)
├── body
│   ├── bytecode: [putself, opt_send_without_block, leave]  // Still here!
│   ├── jit_entry: 0x7f8b2c001000  // Pointer to native machine code
│   └── jit_entry_calls: 1000  // Reached compilation threshold
```

The `jit_entry` field is the gateway to native code. When it's NULL, Ruby interprets bytecode. When it points to compiled code, Ruby jumps directly to machine instructions.

### Key Bytecode Instructions

For understanding execution flow, these instructions are most important:

- `opt_send_without_block` (insns.def:897) - Optimized method call without a block argument
- `send` (insns.def:845) - General method call that can handle blocks
- `leave` - Returns from current method
- `opt_plus`, `opt_minus` - Optimized arithmetic operations

### The Execution Context

Ruby tracks execution state through:
- **Call frames (cfp)** - Each method call creates a frame containing local variables, the instruction pointer, and other method-specific data
- **Stack** - Holds intermediate values during computation
- **Execution context (ec)** - Thread-local state including the current frame pointer

Every method call pushes a new frame, and every return pops one. This stack of frames is central to how Ruby transitions between execution modes.

## Flow 1: Pure Interpreter Execution

When JIT is disabled or a method hasn't been compiled yet, Ruby executes bytecode through the interpreter. This is the baseline execution mode.

### The Interpreter Loop

The heart of Ruby's interpreter is `vm_exec_core()` (vm.c:2639), which implements a classic fetch-decode-execute loop:

1. **Fetch** - Read the next bytecode instruction at the current program counter
2. **Decode** - Determine what operation to perform
3. **Execute** - Perform the operation (might modify stack, call methods, etc.)
4. **Advance** - Move program counter to next instruction

This continues until the method returns or calls another method.

### Method Calls in Detail

When the interpreter encounters a method call like `opt_send_without_block`, here's what happens:

```
┌─────────────────────────────────────────────────────┐
│              vm_exec_core()                          │
│                                                      │
│  PC → [opt_send_without_block :bar]                  │
│                    ↓                                 │
│         vm_sendish() (vm_insnhelper.c:6058)          │
│           1. Look up method 'bar'                    │
│           2. Create new frame for bar()              │
│           3. Push frame onto call stack              │
│           4. Return Qundef (signals "frame ready")   │
│                    ↓                                 │
│          JIT_EXEC macro (vm_exec.h:177)              │
│           - Check if bar has jit_entry               │
│           - In this case: NULL (no JIT)              │
│           - Continue interpreting                    │
│                    ↓                                 │
│        Continue with bar's bytecode                  │
└─────────────────────────────────────────────────────┘
```

The key insight: `vm_sendish()` doesn't execute the called method. It only sets up the new frame and returns a special value (Qundef) meaning "I've pushed a frame, now execute it."

### The JIT_EXEC Check

Even in pure interpreter mode, Ruby always checks for JIT code after pushing a frame. The `JIT_EXEC` macro appears after every `vm_sendish()` call:

```c
// From insns.def:901 (after opt_send_without_block)
val = vm_sendish(ec, cfp, cd, bh, mexp_search_method);
JIT_EXEC(ec, val);  // Always runs, even without JIT enabled
```

The `JIT_EXEC` macro expands to:
```c
if (UNDEF_P(val) &&           // Did vm_sendish return Qundef?
    GET_CFP() != ec->cfp &&    // Did the frame change?
    (func = jit_compile(ec)))  // Is there JIT code?
{
    val = func(ec, ec->cfp);   // Execute JIT code
}
```

When there's no JIT compilation:
- `jit_compile()` checks `iseq->body->jit_entry`
- Finds NULL (no compiled code)
- Returns NULL
- Interpreter continues normally

### Method Returns

The `leave` instruction handles returns:

1. Pop the current frame from the call stack
2. Push the return value onto the caller's stack
3. Continue executing in the caller's frame

The interpreter maintains full control throughout this process.

## Flow 2: Interpreter → JIT Transition

Now let's see what happens when a method has been JIT-compiled and the interpreter needs to transition to native code.

### The JIT_EXEC Gateway

The same `JIT_EXEC` macro we saw earlier becomes the gateway to JIT code. After `vm_sendish()` pushes a new frame, `JIT_EXEC` checks if that frame's method has been compiled:

```c
// vm_exec.h:177
#define JIT_EXEC(ec, val) do { \
    if (UNDEF_P(val) && \
        GET_CFP() != (ec)->cfp && \
        (func = jit_compile(ec))) { \
        val = func(ec, (ec)->cfp); \
    } \
} while (0)
```

All three conditions must be true:
1. **`UNDEF_P(val)`** - vm_sendish returned Qundef (frame was pushed)
2. **`GET_CFP() != ec->cfp`** - The current frame changed (not a tail call)
3. **`func = jit_compile(ec)`** - The current frame's ISEQ has JIT code

### How Methods Get JIT Compiled

Methods don't start with JIT code. They earn it through usage. ZJIT uses a two-phase approach:

```c
// vm.c:437-447
if (body->jit_entry == NULL && rb_zjit_enabled_p) {
    body->jit_entry_calls++;
    
    // First phase: Profile at threshold
    if (body->jit_entry_calls == rb_zjit_profile_threshold) {
        rb_zjit_profile_iseq_entry(iseq, ec);  // Gather runtime info
    }
    // Second phase: Compile at higher threshold
    else if (body->jit_entry_calls == rb_zjit_compile_threshold) {
        rb_zjit_compile_iseq(iseq, ec, false);  // Generate native code
        // After this, iseq->body->jit_entry points to compiled code
    }
}
```

This two-phase approach allows the JIT to:
1. Profile execution patterns before compilation
2. Generate optimized code based on actual runtime behavior

### The Transition Sequence

Let's trace through an actual interpreter-to-JIT transition:

```
Interpreter executing foo() [not JITted]:

1. insns.def:900
   Executes: opt_send_without_block :bar

2. vm_insnhelper.c:6058
   vm_sendish() runs:
   - Looks up bar method
   - Pushes new frame for bar
   - ec->cfp now points to bar's frame
   - Returns Qundef

3. insns.def:901
   JIT_EXEC(ec, val) runs

4. vm.c:431
   jit_compile(ec) runs:
   - Gets current frame's ISEQ (bar's ISEQ)
   - Checks iseq->body->jit_entry
   - Finds 0x7f8b2c001000 (pointer to compiled code)
   - Returns this function pointer

5. vm_exec.h:180
   val = func(ec, ec->cfp)
   - Direct call to native machine code!
   - CPU executes compiled instructions
   - No more bytecode interpretation
```

### Visual: ISEQ State During Transition

Before the call to bar:
```
ISEQ (bar)
├── body
│   ├── jit_entry: 0x7f8b2c001000  ← JIT code exists!
│   ├── jit_entry_calls: 1000       ← Reached threshold
│   └── bytecode: [putobject 42, leave]  ← Still available
```

During the transition:
```
Call Stack:          Execution Mode:
[foo frame]    →     Interpreter (executing opt_send_without_block)
     ↓
[bar frame]    →     JIT (about to execute native code at 0x7f8b2c001000)
```

The transition is remarkably simple - it's just a function pointer call. The interpreter calls `func(ec, ec->cfp)`, which jumps directly to the machine code address stored in `jit_entry`.

### What Happens in JIT Code

Once execution transfers to JIT code:
- Native CPU instructions execute directly
- No instruction fetch/decode overhead
- Operations can be inlined and optimized
- Register allocation instead of stack manipulation
- Type-specialized fast paths

The JIT code has full access to the Ruby VM's data structures and can:
- Call other JITted methods directly
- Fall back to the interpreter when needed
- Manipulate the stack and frames just like the interpreter would

## Flow 3: JIT → Interpreter (Deoptimization)

JIT code can't always continue executing natively. When it encounters situations it can't handle, it gracefully returns control to the interpreter. The most common reason is guard failures.

### Type Guards and Optimized Code

JIT compilers optimize code by making assumptions. Consider this method:

```ruby
def add(a, b)
  a + b
end
```

The bytecode uses `opt_plus`:
```
== disasm: #<ISeq:add@-e:1 (1,0)-(1,25)>
0000 getlocal_WC_0                          a@0
0002 getlocal_WC_0                          b@1
0004 opt_plus                               <calldata!mid:+, argc:1>
0006 leave
```

When ZJIT compiles this method after profiling shows it's always called with Fixnums (small integers), it generates optimized code:

```c
// Simplified representation of what ZJIT generates:
// 1. Type guard for 'a'
if (!FIXNUM_P(a)) goto side_exit;

// 2. Type guard for 'b'  
if (!FIXNUM_P(b)) goto side_exit;

// 3. Fast path: direct integer addition
result = INT2FIX(FIX2INT(a) + FIX2INT(b));
return result;
```

The JIT inserts type guards using instructions like:
```rust
// From zjit/src/hir.rs:1567
Insn::GuardType { val, guard_type: types::Fixnum, state }
```

### Guard Failure Flow

What happens when assumptions break:

```ruby
# First 1000 calls - guards pass
add(1, 2)      # Both Fixnums ✓
add(5, 10)     # Both Fixnums ✓

# Call 1001 - guard fails!
add(1.5, 2)    # 1.5 is Float, not Fixnum ✗
```

When the type guard fails:

```
JIT Code (add method):
├─ Load 'a' (value: 1.5)
├─ Check: is 'a' Fixnum?
│  └─ NO! It's a Float
├─ Jump to side exit ←── Deoptimization starts here
│
Side Exit:
├─ Restore interpreter state:
│  ├─ Update cfp->sp (stack pointer)
│  ├─ Update cfp->pc (program counter) 
│  └─ Set return value to Qundef
└─ Return to interpreter
```

### The VM_EXEC Macro

When JIT code returns control to the interpreter, it uses the `VM_EXEC` macro:

```c
// vm_exec.h:169
#define VM_EXEC(ec, val) do { \
    if (UNDEF_P(val)) { \
        VM_ENV_FLAGS_SET((ec)->cfp->ep, VM_FRAME_FLAG_FINISH); \
        val = vm_exec(ec);  /* Re-enter interpreter */ \
    } \
} while (0)
```

The macro:
1. Checks if JIT returned Qundef (signal for "need interpreter")
2. Sets a finish flag on the current frame
3. Calls `vm_exec()` to continue in interpreter mode

### Visual: Complete Deoptimization Flow

```
Initial State:
  JIT executing add(1.5, 2)
         ↓
Type Guard Check:
  FIXNUM_P(1.5) → false
         ↓
Side Exit:
  ├─ Save registers to stack
  ├─ cfp->pc = &opt_plus instruction
  ├─ cfp->sp = correct stack position
  └─ return Qundef
         ↓
VM_EXEC macro:
  UNDEF_P(val) → true
  vm_exec(ec) called
         ↓
Interpreter:
  Executes opt_plus with Float + Fixnum
  (slower but correct path)
```

### Other Deoptimization Triggers

Besides type guards, JIT returns to interpreter for:

1. **Calling non-JITted methods** - If the target method hasn't been compiled
2. **Unsupported bytecode** - Some Ruby features are too complex to JIT
3. **Redefined core methods** - If someone redefines `+` on Integer
4. **Maximum optimization attempts** - After too many recompilations

The key insight: deoptimization isn't failure - it's a normal part of Ruby's execution strategy. The VM seamlessly handles the transition, ensuring your code always runs correctly even when optimistic assumptions prove wrong.

## Performance Implications

### JIT Compilation Timeline

Here's how a typical method evolves from interpreted to JIT-compiled:

```
Method 'calculate' lifecycle:

Calls:     0 ────────── 100 ────────── 1000 ────────── 10000+
           │            │              │               │
Mode:      └─Interpret──┴──Profile────┴──JIT Compile──┴──Native Code
           
Timeline:  Cold start   Gathering     Optimization    Peak perf
           (slower)     runtime data  happens         (fastest)
```

Key thresholds (ZJIT defaults):
- **Profile threshold**: ~100 calls - Start gathering type information
- **Compile threshold**: ~1000 calls - Generate optimized native code
- **Steady state**: 1000+ calls - Execute compiled code

### Cost of Transitions

Transitions between execution modes are remarkably cheap:

**Interpreter → JIT:**
- Just a function pointer call
- No state copying needed (same stack/frames)
- Cost: ~10-20 CPU cycles (negligible)

**JIT → Interpreter (deoptimization):**
- Restore stack pointer and program counter
- Jump back to vm_exec
- Cost: ~100-200 CPU cycles (still minimal)

Compare this to the savings:
- Interpreted instruction: ~50-100 cycles each
- JIT instruction: ~1-5 cycles each
- A hot loop with 100 iterations saves thousands of cycles per execution

### Why Mixed Execution Works Well

Ruby's approach has several advantages:

1. **Gradual optimization** - Only hot code gets compiled
2. **Memory efficiency** - Cold code stays as compact bytecode
3. **Correctness first** - Guards ensure optimization never breaks semantics
4. **Adaptive** - Can reoptimize based on changing patterns

Most Ruby applications have a natural hot/cold code distribution:
- ~10% of methods are hot (called thousands of times)
- ~90% are cold (called rarely)
- JIT focuses effort where it matters most

## Key Functions Reference

### Core Execution Functions

| Function | Location | Purpose |
|----------|----------|---------|
| `vm_exec()` | vm.c:2638 | Main entry point, orchestrates execution |
| `jit_exec()` | vm.c:467 | Attempts to execute JIT code if available |
| `vm_exec_core()` | vm.c:2639 | The interpreter loop |
| `vm_sendish()` | vm_insnhelper.c:6058 | Handles method calls, pushes frames |

### Transition Mechanisms

| Macro/Function | Location | Direction | When Used |
|----------------|----------|-----------|-----------|
| `JIT_EXEC` | vm_exec.h:177 | Interpreter → JIT | After every method call |
| `VM_EXEC` | vm_exec.h:169 | JIT → Interpreter | On deoptimization |
| `jit_compile()` | vm.c:431 | - | Retrieves JIT function pointer |

### Method Call Instructions

Both these instructions follow the same execution path through `vm_sendish()` and `JIT_EXEC`:

| Instruction | Location | Usage |
|-------------|----------|-------|
| `opt_send_without_block` | insns.def:897 | Method calls without blocks (most common) |
| `send` | insns.def:845 | Method calls with block support |

### ZJIT-Specific Functions

| Function | Purpose |
|----------|---------|
| `rb_zjit_profile_iseq_entry()` | Gathers runtime type information |
| `rb_zjit_compile_iseq()` | Generates optimized native code |

## Takeaways

Understanding Ruby's execution flow reveals several important insights:

1. **Ruby seamlessly transitions between interpreted and native code** - The same `JIT_EXEC` check happens after every method call, making transitions automatic and transparent.

2. **JIT compilation is incremental based on usage** - Methods earn compilation through repeated calls, ensuring effort is spent where it provides value.

3. **Type guards ensure correctness while enabling optimization** - The JIT can make aggressive optimizations because guards guarantee assumptions hold.

4. **Deoptimization gracefully handles edge cases** - When guards fail or unsupported features are encountered, the interpreter takes over seamlessly.

5. **Both execution modes share the same VM structures** - ISEQs, frames, and the stack work identically in both modes, enabling smooth transitions.

6. **The execution flow is the same for YJIT and ZJIT** - While compilation strategies differ, the fundamental execution patterns described here apply to both JITs.

This execution architecture allows Ruby to be both dynamic and performant. Your code starts interpreted for flexibility, hot paths get optimized for speed, and the VM ensures everything runs correctly regardless of execution mode.

The next time your Ruby application runs in production with JIT enabled, you'll know exactly how it's transitioning between interpreted bytecode and native machine code - potentially thousands of times per second, all transparently managed by the Ruby VM.
